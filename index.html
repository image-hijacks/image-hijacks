<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Image hijacks — specially trained adversarial images — can control the behaviour of vision-language models at runtime.">
  <meta name="keywords"
    content="image hijacks, image hijacking, behaviour matching, adversarial attack, vision-language model, multimodal model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Image Hijacks: Adversarial Images can Control Generative Models at Runtime</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PQ1B9ZGBP7"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-PQ1B9ZGBP7');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="https://humancompatible.ai/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css"
    integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"
    integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx"
    crossorigin="anonymous"></script>

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <!-- <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://keunhong.com">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://hypernerf.github.io">
              HyperNeRF
            </a>
            <a class="navbar-item" href="https://nerfies.github.io">
              Nerfies
            </a>
            <a class="navbar-item" href="https://latentfusion.github.io">
              LatentFusion
            </a>
            <a class="navbar-item" href="https://photoshape.github.io">
              PhotoShape
            </a>
          </div>
        </div>
      </div>

    </div> -->
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Image Hijacks: Adversarial Images can Control Generative Models
              at Runtime</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://twitter.com/LukeBailey181">Luke
                  Bailey</a>* <!--; color: #cc00d7 !important-->
                <sup>1,2</sup>,</span>
              <span class="author-block">
                <a href="https://ong.ac">Euan Ong</a>* <!--; color: #f68946 !important-->
                <sup>1,3</sup>,</span>
              <span class="author-block">
                <a href="http://people.eecs.berkeley.edu/~russell/">Stuart Russell</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="http://scottemmons.com">Scott Emmons</a><sup>1</sup>
              </span>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>UC Berkeley,</span>
              <span class="author-block"><sup>2</sup>Harvard University,</span>
              <span class="author-block"><sup>3</sup>University of Cambridge</span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block">* denotes equal contribution</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2309.00236.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2309.00236" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!--Video Link.-->
                <!-- <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/euanong/image-hijacks"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link.
                <span class="link-block">
                  <a href="https://github.com/google/nerfies/releases/tag/0.1"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a> -->
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
          free-viewpoint
          portraits.
        </h2>
      </div>
    </div>
  </section> -->


  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="content has-text-justified" style="padding-top: 10px; padding-bottom: 10px">
              <img src="./static/images/overview.svg" alt="Interpolate start reference image."
                style="padding-bottom: 10px;" />
              <p><b>Figure 1</b>: Image hijacks of LLaVA-2, a VLM based on CLIP and LLaMA-2. These attacks are
                automated, barely perceptible to humans, and control the model's output.</p>
            </div>
          </div>
        </div>

      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <!-- <p>
              Are foundation models secure from malicious actors?
            </p>
            <p>
              In this work, we study the attack surface of vision-language models (VLMs). We discover that their image
              input channel is vulnerable to attack, by way of <b>image
                hijacks</b>: adversarial images that control
              generative models at runtime.
            </p>
            <p>
              We introduce <b>behaviour matching</b>, a general method for crafting image
              hijacks, and use it to build three different types of attack:
            <ul>
              <li><b>Specific string attacks</b> force a model to generate arbitrary output of the adversary's choosing.
              </li>
              <li><b>Leak context attacks</b> force a model to leak information from its context window into its output.
              </li>
              <li><b>Jailbreak attacks</b> circumvent a model's safety training.</li>
            </ul>
            </p>
            <p>
              We study these attacks
              against <a href="https://llava-vl.github.io/">LLaVA-2</a>, a state-of-the-art VLM based on CLIP and
              LLaMA-2, and find that <b>all our attack types have
                above a 90% success rate</b>. Moreover, our attacks are <b>automated</b> and require only <b>small image
                perturbations</b>.
            </p>
            <p>
              These findings raise serious concerns about the security of foundation models: if image hijacks are as
              difficult to defend against as adversarial examples in CIFAR-10, then it might be many years before a
              solution is found — if one even exists.
            </p> -->
            <p>
              Are foundation models secure against malicious actors? 
            </p>
            <p>
              In this work, we focus on the image input to a vision-language model (VLM). We discover that their image input channel is vulnerable to attack, by way of <b>image hijacks</b>: adversarial images that control
              generative models at runtime. 
            </p>
            <p>
              We introduce the general <b>behaviour-matching</b> algorithm for training image hijacks. From this, we  derive the <b>prompt-matching</b> algorithm, allowing us to train hijacks matching the behaviour of an <emph>arbitrary user-defined text prompt</emph> (e.g. 'the Eiffel Tower is now located in Rome') using a generic, off-the-shelf dataset <emph>unrelated to our choice of prompt</emph>. 
            </p>
            <p>
              We use behaviour-matching to craft hijacks for four types of attack:
              <ul>
                <li><b>Specific string attacks</b> force a model to generate arbitrary output of the adversary's choosing.
                </li>
                <li><b>Leak context attacks</b> force a model to leak information from its context window into its output.
                </li>
                <li><b>Jailbreak attacks</b> circumvent a model's safety training.</li>
                <li><b>Disinformation attacks</b> force a model to believe false information.</li>
              </ul>
            </p>  
            <p>
              We study these attacks against <a href="https://llava-vl.github.io/">LLaVA</a>, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that <b>all attack types achieve a success rate of over 80%</b>. Moreover, our attacks are <b>automated</b> and require only <b>small image perturbations</b>. 
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!--<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Demo</h2>
          <div class="content has-text-justified">
            <p>
              While the experiments in our paper were performed on the <a
                href="https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md">latest version of LLaVA</a>
              based on LLaMA-2, we also trained specific string and leak context attacks for the <a
                href="https://llava.hliu.cc">public LLaVA
                demo</a>, which you can try below:
            </p>
          </div>
          <div class="columns">
            <div class="column is-4 has-text-centered">
              <img src="./static/images/original.png" class="interpolation-image"
                alt="Interpolate start reference image.">
              <p>Original image</p>
            </div>
            <div class="column is-4 has-text-centered">
              <img src="./static/images/leak_context_attack.png" class="interpolation-image"
                alt="Interpolate start reference image.">
              <p>Leak-context hijack under \(\ell_\infty\) norm constraint (\(\varepsilon=8/255\))</p>
            </div>
            <div class="column is-4 has-text-centered">
              <img src="./static/images/specific_string_attack.png" class="interpolation-image"
                alt="Interpolation end reference image.">
              <p>Specific-string hijack under \(\ell_\infty\) norm constraint (\(\varepsilon=8/255\))</p>
            </div>
          </div>
        </div>
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <iframe src="https://euanong-image-hijacks-demo.hf.space/" style="height: 900px;width: min(1200px, 80vw);">

          </iframe>
        </div>
      </div>


    </div>
  </section>-->


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-3" style="text-align: center;">BibTeX</h2>
      <pre style="margin-left: 100px; margin-right: 100px"><code>@misc{bailey2023image,
  title={Image Hijacks: Adversarial Images can Control Generative Models at Runtime}, 
  author={Luke Bailey and Euan Ong and Stuart Russell and Scott Emmons},
  year={2023},
  eprint={2309.00236},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <!-- <div class="content has-text-centered">
        <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div> -->
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is inspired by the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies project
                page</a>, and licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a
                href="https://github.com/image-hijacks/image-hijacks.github.io">source
                code</a> of this website;
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
  <script src="./static/js/mockframe.js"></script>
  <script>
    newMockFrame();
  </script>
</body>

</html>
